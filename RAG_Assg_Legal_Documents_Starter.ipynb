{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Extracting Information from Legal Documents Using RAG**"
      ],
      "metadata": {
        "id": "lf5lYawIw8tE"
      },
      "id": "lf5lYawIw8tE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Objective**"
      ],
      "metadata": {
        "id": "NY1InIbkw80B"
      },
      "id": "NY1InIbkw80B"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3403a4b5"
      },
      "source": [
        "The main objective of this assignment is to process and analyse a collection text files containing legal agreements (e.g., NDAs) to prepare them for implementing a **Retrieval-Augmented Generation (RAG)** system. This involves:\n",
        "\n",
        "* Understand the Cleaned Data : Gain a comprehensive understanding of the structure, content, and context of the cleaned dataset.\n",
        "* Perform Exploratory Analysis : Conduct bivariate and multivariate analyses to uncover relationships and trends within the cleaned data.\n",
        "* Create Visualisations : Develop meaningful visualisations to support the analysis and make findings interpretable.\n",
        "* Derive Insights and Conclusions : Extract valuable insights from the cleaned data and provide clear, actionable conclusions.\n",
        "* Document the Process : Provide a detailed description of the data, its attributes, and the steps taken during the analysis for reproducibility and clarity.\n",
        "\n",
        "The ultimate goal is to transform the raw text data into a clean, structured, and analysable format that can be effectively used to build and train a RAG system for tasks like information retrieval, question-answering, and knowledge extraction related to legal agreements."
      ],
      "id": "3403a4b5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Business Value**  \n"
      ],
      "metadata": {
        "id": "3TTEcbb5hIM-"
      },
      "id": "3TTEcbb5hIM-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project aims to leverage RAG to enhance legal document processing for businesses, law firms, and regulatory bodies. The key business objectives include:\n",
        "\n",
        "* Faster Legal Research: <br> Reduce the time lawyers and compliance officers spend searching for relevant case laws, precedents, statutes, or contract clauses.\n",
        "* Improved Contract Analysis: <br> Automatically extract key terms, obligations, and risks from lengthy contracts.\n",
        "* Regulatory Compliance Monitoring: <br> Help businesses stay updated with legal and regulatory changes by retrieving relevant legal updates.\n",
        "* Enhanced Decision-Making: <br> Provide accurate and context-aware legal insights to assist in risk assessment and legal strategy.\n",
        "\n",
        "\n",
        "**Use Cases**\n",
        "* Legal Chatbots\n",
        "* Contract Review Automation\n",
        "* Tracking Regulatory Changes and Compliance Monitoring\n",
        "* Case Law Analysis of past judgments\n",
        "* Due Diligence & Risk Assessment"
      ],
      "metadata": {
        "id": "ZsfkEL2CgljF"
      },
      "id": "ZsfkEL2CgljF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Data Loading, Preparation and Analysis** <font color=red> [20 marks] </font><br>"
      ],
      "metadata": {
        "id": "rDp_EWxVOhUu"
      },
      "id": "rDp_EWxVOhUu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.1 Data Understanding**"
      ],
      "metadata": {
        "id": "JZGTCfyUxalZ"
      },
      "id": "JZGTCfyUxalZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains legal documents and contracts collected from various sources. The documents are present as text files (`.txt`) in the *corpus* folder.\n",
        "\n",
        "There are four types of documents in the *courpus* folder, divided into four subfolders.\n",
        "- `contractnli`: contains various non-disclosure and confidentiality agreements\n",
        "- `cuad`: contains contracts with annotated legal clauses\n",
        "- `maud`: contains various merger/acquisition contracts and agreements\n",
        "- `privacy_qa`: a question-answering dataset containing privacy policies\n",
        "\n",
        "The dataset also contains evaluation files in JSON format in the *benchmark* folder. The files contain the questions and their answers, along with sources. For each of the above four folders, there is a `json` file: `contractnli.json`, `cuad.json`, `maud.json` `privacy_qa.json`. The file structure is as follows:\n",
        "\n",
        "```\n",
        "{\n",
        "    \"tests\": [\n",
        "        {\n",
        "            \"query\": <question1>,\n",
        "            \"snippets\": [{\n",
        "                    \"file_path\": <source_file1>,\n",
        "                    \"span\": [ begin_position, end_position ],\n",
        "                    \"answer\": <relevant answer to the question 1>\n",
        "                },\n",
        "                {\n",
        "                    \"file_path\": <source_file2>,\n",
        "                    \"span\": [ begin_position, end_position ],\n",
        "                    \"answer\": <relevant answer to the question 2>\n",
        "                }, ....\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"query\": <question2>,\n",
        "            \"snippets\": [{<answer context for que 2>}]\n",
        "        },\n",
        "        ... <more queries>\n",
        "    ]\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "ok6sSYNAiG8V"
      },
      "id": "ok6sSYNAiG8V"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.2 Load and Preprocess the data** <font color=red> [5 marks] </font><br>"
      ],
      "metadata": {
        "id": "S7Ac8VxvjWnw"
      },
      "id": "S7Ac8VxvjWnw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading libraries"
      ],
      "metadata": {
        "id": "gJ8fA4Nh3fHg"
      },
      "id": "gJ8fA4Nh3fHg"
    },
    {
      "source": [
        "## The following libraries might be useful\n",
        "# !pip install -q langchain-openai\n",
        "# !pip install -U -q langchain-community\n",
        "# !pip install -U -q langchain-chroma\n",
        "# !pip install -U -q datasets\n",
        "# !pip install -U -q ragas\n",
        "# !pip install -U -q rouge_score"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "BqyFHhSn48tC"
      },
      "execution_count": 22,
      "outputs": [],
      "id": "BqyFHhSn48tC"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import essential libraries\n",
        "!pip install langchain llama-index openai rouge-score ragas evaluate nltk scikit-learn tiktoken\n",
        "!pip install git+https://github.com/explodinggradients/ragas.git\n"
      ],
      "metadata": {
        "id": "Qpn-qbhAi58F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97c0ae71-4647-414d-fed6-2ac29543320e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.11/dist-packages (0.12.44)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.91.0)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: ragas in /usr/local/lib/python3.11/dist-packages (0.2.15)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.66)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.5,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.12)\n",
            "Requirement already satisfied: llama-index-cli<0.5,>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.3)\n",
            "Requirement already satisfied: llama-index-core<0.13,>=0.12.44 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.12.44)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.1)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.7.7)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.5,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.7)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.6,>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.5.1)\n",
            "Requirement already satisfied: llama-index-program-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.2)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.1)\n",
            "Requirement already satisfied: llama-index-readers-file<0.5,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.9)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from ragas) (2.14.4)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (from ragas) (0.3.26)\n",
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.11/dist-packages (from ragas) (0.3.27)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ragas) (1.6.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.11/dist-packages (from ragas) (1.4.4)\n",
            "Requirement already satisfied: diskcache>=5.6.3 in /usr/local/lib/python3.11/dist-packages (from ragas) (5.6.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.23.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->ragas) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->ragas) (3.11.15)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (2.1.3)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (1.2.0)\n",
            "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (1.0.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (3.5)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (11.2.1)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (80.9.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (1.17.2)\n",
            "Requirement already satisfied: llama-cloud==0.1.26 in /usr/local/lib/python3.11/dist-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.26)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (4.13.4)\n",
            "Requirement already satisfied: pypdf<6,>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (5.6.1)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.34)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community->ragas) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community->ragas) (0.4.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->ragas) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->ragas) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->ragas) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->ragas) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->ragas) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->ragas) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->ragas) (1.20.1)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.44->llama-index) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.44->llama-index) (3.1.6)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.44->llama-index) (4.3.8)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.7)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13,>=0.12.44->llama-index) (3.26.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.13,>=0.12.44->llama-index) (0.2.0)\n",
            "Requirement already satisfied: llama-cloud-services>=0.6.32 in /usr/local/lib/python3.11/dist-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.34)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community->ragas) (1.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.44->llama-index) (1.1.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.44->llama-index) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.44->llama-index) (3.0.2)\n",
            "Collecting git+https://github.com/explodinggradients/ragas.git\n",
            "  Cloning https://github.com/explodinggradients/ragas.git to /tmp/pip-req-build-fb3z1yfz\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/explodinggradients/ragas.git /tmp/pip-req-build-fb3z1yfz\n",
            "  Resolved https://github.com/explodinggradients/ragas.git to commit daa8ca20f1bb2592a3eeadd8364bb55ca0934a99\n",
            "\u001b[31mERROR: git+https://github.com/explodinggradients/ragas.git does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "id": "Qpn-qbhAi58F"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.2.1** <font color=red> [3 marks] </font>\n",
        "Load all `.txt` files from the folders."
      ],
      "metadata": {
        "id": "zOMf-tfIiOlp"
      },
      "id": "zOMf-tfIiOlp"
    },
    {
      "cell_type": "markdown",
      "id": "f2ea36ba",
      "metadata": {
        "id": "f2ea36ba"
      },
      "source": [
        "You can utilise document loaders from the options provided by the LangChain community.\n",
        "\n",
        "Optionally, you can also read the files manually, while ensuring proper handling of encoding issues (e.g., utf-8, latin1). In such case, also store the file content along with metadata (e.g., file name, directory path) for traceability."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Step 1: Set path for the uploaded ZIP file and target extraction directory\n",
        "zip_path = \"/content/rag_legal.zip\"\n",
        "extract_dir = \"/content/rag_legal\"  # this will be your root dataset folder\n",
        "\n",
        "# Step 2: Unzip the file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "print(\"✅ Dataset unzipped at:\", extract_dir)\n"
      ],
      "metadata": {
        "id": "I9rTY8DWx2Wj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b609789-f87f-4b09-8395-46ebf35b5512"
      },
      "id": "I9rTY8DWx2Wj",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset unzipped at: /content/rag_legal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir(\"/content/rag_legal\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1F5sfmcuG0i",
        "outputId": "78d103a8-7707-4e62-c531-ec9657bc6c25"
      },
      "id": "W1F5sfmcuG0i",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rag_legal']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the files as documents\n",
        "import glob\n",
        "import os\n",
        "\n",
        "def load_all_txt_documents(folder_path):\n",
        "    all_docs = []\n",
        "\n",
        "    # Recursively search for all .txt files\n",
        "    for file_path in glob.glob(os.path.join(folder_path, '**', '*.txt'), recursive=True):\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                text = f.read()\n",
        "                all_docs.append({\n",
        "                    \"text\": text,\n",
        "                    \"file_path\": file_path,\n",
        "                    \"category\": os.path.basename(os.path.dirname(file_path))\n",
        "                })\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error reading {file_path}: {e}\")\n",
        "\n",
        "    return all_docs\n",
        "\n",
        "corpus_path = \"/content/rag_legal\"\n",
        "documents = load_all_txt_documents(corpus_path)\n",
        "\n",
        "print(f\"✅ Loaded {len(documents)} documents\")\n",
        "print(\"📄 Sample document keys:\", documents[0].keys() if documents else \"No documents found\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6igN8xRs543",
        "outputId": "7a7862e5-16c7-4938-fc6d-c8839e969e83"
      },
      "id": "n6igN8xRs543",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded 698 documents\n",
            "📄 Sample document keys: dict_keys(['text', 'file_path', 'category'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.2.2** <font color=red> [2 marks] </font>\n",
        "Preprocess the text data to remove noise and prepare it for analysis."
      ],
      "metadata": {
        "id": "K4HYLoUjwmMs"
      },
      "id": "K4HYLoUjwmMs"
    },
    {
      "cell_type": "markdown",
      "id": "e9793fdf",
      "metadata": {
        "id": "e9793fdf"
      },
      "source": [
        "Remove special characters, extra whitespace, and irrelevant content such as email and telephone contact info.\n",
        "Normalise text (e.g., convert to lowercase, remove stop words).\n",
        "Handle missing or corrupted data by logging errors and skipping problematic files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "1ec87e69",
      "metadata": {
        "id": "1ec87e69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0fa9e44-b8e1-4b21-9924-ae6ba0119415"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Clean and preprocess the data\n",
        "\n",
        "# STEP 1: Imports and stopwords setup\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from langchain.schema import Document\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# STEP 2: Text cleaning function\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)  # Remove emails\n",
        "    text = re.sub(r'(\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4})', '', text)  # Remove phone numbers\n",
        "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)  # Remove special characters\n",
        "    text = text.lower()\n",
        "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "# STEP 3: Create LangChain documents from preprocessed raw data\n",
        "# documents already exists with keys: 'text', 'file_path', 'category'\n",
        "docs = [\n",
        "    Document(\n",
        "        page_content=preprocess_text(doc['text']),\n",
        "        metadata={\"source\": doc[\"file_path\"], \"category\": doc[\"category\"]}\n",
        "    )\n",
        "    for doc in documents\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9e90470"
      },
      "source": [
        "### **1.3 Exploratory Data Analysis** <font color=red> [10 marks] </font><br>"
      ],
      "id": "b9e90470"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.3.1** <font color=red> [1 marks] </font>\n",
        "Calculate the average, maximum and minimum document length."
      ],
      "metadata": {
        "id": "Nd1K4yhIzyPp"
      },
      "id": "Nd1K4yhIzyPp"
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average, maximum and minimum document length.\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "lengths = [len(doc.page_content.split()) for doc in docs]\n",
        "print(f\"Average Length: {np.mean(lengths)}\\nMax Length: {np.max(lengths)}\\nMin Length: {np.min(lengths)}\")"
      ],
      "metadata": {
        "id": "tQT1UIcOHSp9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f6672d8-0327-461b-f8df-8ebe1e7f4eac"
      },
      "id": "tQT1UIcOHSp9",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Length: 8975.432664756447\n",
            "Max Length: 84946\n",
            "Min Length: 142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.3.2** <font color=red> [4 marks] </font>\n",
        "Analyse the frequency of occurrence of words and find the most and least occurring words."
      ],
      "metadata": {
        "id": "18xQu__O0wLv"
      },
      "id": "18xQu__O0wLv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the 20 most common and least common words in the text. Ignore stop words such as articles and prepositions."
      ],
      "metadata": {
        "id": "IQ_i5YfFH2dg"
      },
      "id": "IQ_i5YfFH2dg"
    },
    {
      "cell_type": "code",
      "source": [
        "# Find frequency of occurence of words\n",
        "all_words = ' '.join(doc.page_content for doc in docs).split()\n",
        "counter = Counter(all_words)\n",
        "\n",
        "print(\"20 Most Common Words:\", counter.most_common(20))\n",
        "print(\"20 Least Common Words:\", counter.most_common()[-20:])"
      ],
      "metadata": {
        "id": "Q8eiDTy2Ic8z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "727c6e59-d21f-4bad-ae22-0e5e9d6232c4"
      },
      "id": "Q8eiDTy2Ic8z",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20 Most Common Words: [('company', 148170), ('shall', 107995), ('agreement', 104559), ('section', 75344), ('parent', 58009), ('party', 49657), ('date', 39294), ('time', 35251), ('material', 34208), ('merger', 33843), ('subsidiaries', 33317), ('applicable', 31369), ('including', 29398), ('respect', 28848), ('may', 28069), ('stock', 26651), ('information', 25681), ('parties', 24610), ('b', 23935), ('business', 23497)]\n",
            "20 Least Common Words: [('122aib', 1), ('122di', 1), ('122ai', 1), ('122aiia', 1), ('122e', 1), ('123ciia2', 1), ('123ciia3', 1), ('123ciib1', 1), ('123ciib2', 1), ('123ciib3', 1), ('123ciii', 1), ('trialbytrial', 1), ('perdetail', 1), ('123cvide', 1), ('123cvi', 1), ('122aiii', 1), ('123di', 1), ('122aiib', 1), ('takings', 1), ('exhibitsancillary', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.3.3** <font color=red> [4 marks] </font>\n",
        "Analyse the similarity of different documents to each other based on TF-IDF vectors."
      ],
      "metadata": {
        "id": "xlF55RNjz9pQ"
      },
      "id": "xlF55RNjz9pQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform some documents to TF-IDF vectors and calculate their similarity matrix using a suitable distance function. If contracts contain duplicate or highly similar clauses, similarity calculation can help detect them.\n",
        "\n",
        "Identify for the first 10 documents and then for 10 random documents. What do you observe?"
      ],
      "metadata": {
        "id": "jciCNMelOGPJ"
      },
      "id": "jciCNMelOGPJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the page contents of documents\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import random\n",
        "\n",
        "contents = [doc.page_content for doc in docs]\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(contents)\n",
        "\n",
        "# Compute similarity scores\n",
        "\n",
        "# First 10 docs\n",
        "similarity_matrix_first10 = cosine_similarity(tfidf_matrix[:10])\n",
        "print(similarity_matrix_first10)\n"
      ],
      "metadata": {
        "id": "M-_SrvDcMnKi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6709422c-8e1b-4556-d4a7-42162a206892"
      },
      "id": "M-_SrvDcMnKi",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.29298252 0.30119917 0.19987952 0.27135081 0.19520498\n",
            "  0.17481365 0.10513746 0.05580197 0.06503493]\n",
            " [0.29298252 1.         0.40471471 0.32220309 0.42714379 0.28313062\n",
            "  0.31475707 0.13116893 0.05755453 0.09096209]\n",
            " [0.30119917 0.40471471 1.         0.35764548 0.52624268 0.31982757\n",
            "  0.24753857 0.12046101 0.06402482 0.07828118]\n",
            " [0.19987952 0.32220309 0.35764548 1.         0.34305073 0.20130963\n",
            "  0.19943237 0.06875807 0.03090197 0.04641382]\n",
            " [0.27135081 0.42714379 0.52624268 0.34305073 1.         0.30138468\n",
            "  0.26771973 0.10962853 0.05489759 0.07350297]\n",
            " [0.19520498 0.28313062 0.31982757 0.20130963 0.30138468 1.\n",
            "  0.17732837 0.08294927 0.03999159 0.05439024]\n",
            " [0.17481365 0.31475707 0.24753857 0.19943237 0.26771973 0.17732837\n",
            "  1.         0.06451441 0.03113781 0.04363283]\n",
            " [0.10513746 0.13116893 0.12046101 0.06875807 0.10962853 0.08294927\n",
            "  0.06451441 1.         0.11276923 0.11385125]\n",
            " [0.05580197 0.05755453 0.06402482 0.03090197 0.05489759 0.03999159\n",
            "  0.03113781 0.11276923 1.         0.05786889]\n",
            " [0.06503493 0.09096209 0.07828118 0.04641382 0.07350297 0.05439024\n",
            "  0.04363283 0.11385125 0.05786889 1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a list of 10 random integers\n",
        "random_indices = random.sample(range(len(contents)), 10)\n"
      ],
      "metadata": {
        "id": "pd99eXtnK2DU"
      },
      "id": "pd99eXtnK2DU",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute similarity scores for 10 random documents\n",
        "similarity_matrix_random10 = cosine_similarity(tfidf_matrix[random_indices])\n",
        "print(similarity_matrix_random10)"
      ],
      "metadata": {
        "id": "t31ngfZTJimS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf06328c-616e-4e16-c945-12fe7a145643"
      },
      "id": "t31ngfZTJimS",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.0757301  0.13947774 0.1187125  0.32801922 0.16913765\n",
            "  0.06886358 0.06086552 0.24267132 0.12255768]\n",
            " [0.0757301  1.         0.10467891 0.09211151 0.10088128 0.03822613\n",
            "  0.0532536  0.05155355 0.13454115 0.09852918]\n",
            " [0.13947774 0.10467891 1.         0.16961128 0.20049042 0.07492238\n",
            "  0.10470828 0.08798864 0.22253915 0.42256896]\n",
            " [0.1187125  0.09211151 0.16961128 1.         0.37648659 0.09629327\n",
            "  0.1010073  0.10773696 0.30676399 0.16787597]\n",
            " [0.32801922 0.10088128 0.20049042 0.37648659 1.         0.30297018\n",
            "  0.10307882 0.1087616  0.45611776 0.19638037]\n",
            " [0.16913765 0.03822613 0.07492238 0.09629327 0.30297018 1.\n",
            "  0.03233631 0.03169758 0.1704716  0.07605157]\n",
            " [0.06886358 0.0532536  0.10470828 0.1010073  0.10307882 0.03233631\n",
            "  1.         0.0566992  0.13616306 0.11665981]\n",
            " [0.06086552 0.05155355 0.08798864 0.10773696 0.1087616  0.03169758\n",
            "  0.0566992  1.         0.13577851 0.11283305]\n",
            " [0.24267132 0.13454115 0.22253915 0.30676399 0.45611776 0.1704716\n",
            "  0.13616306 0.13577851 1.         0.25973173]\n",
            " [0.12255768 0.09852918 0.42256896 0.16787597 0.19638037 0.07605157\n",
            "  0.11665981 0.11283305 0.25973173 1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cfd0f53"
      },
      "source": [
        "### **1.4 Document Creation and Chunking** <font color=red> [5 marks] </font><br>"
      ],
      "id": "3cfd0f53"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.4.1** <font color=red> [5 marks] </font>\n",
        "Perform appropriate steps to split the text into chunks."
      ],
      "metadata": {
        "id": "pCw3NzcE3waS"
      },
      "id": "pCw3NzcE3waS"
    },
    {
      "cell_type": "code",
      "source": [
        "# Process files and generate chunks\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "chunks = splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "TjZ6yf9r2p1F"
      },
      "execution_count": 33,
      "outputs": [],
      "id": "TjZ6yf9r2p1F"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Vector Database and RAG Chain Creation** <font color=red> [15 marks] </font><br>"
      ],
      "metadata": {
        "id": "LeAeTqpZ-DYw"
      },
      "id": "LeAeTqpZ-DYw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoH_Ac6K6aQZ"
      },
      "source": [
        "### **2.1 Vector Embedding and Vector Database Creation** <font color=red> [7 marks] </font><br>"
      ],
      "id": "YoH_Ac6K6aQZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.1.1** <font color=red> [2 marks] </font>\n",
        "Initialise an embedding function for loading the embeddings into the vector database."
      ],
      "metadata": {
        "id": "bBfj5ycC59lU"
      },
      "id": "bBfj5ycC59lU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialise a function to transform the text to vectors using OPENAI Embeddings module. You can also use this function to transform during vector DB creation itself."
      ],
      "metadata": {
        "id": "v-QeR5N_7jiw"
      },
      "id": "v-QeR5N_7jiw"
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch your OPENAI API Key as an environment variable\n",
        "!pip install -q langchain faiss-cpu transformers datasets evaluate rouge-score nltk\n",
        "!pip install sentence-transformers\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "id": "b3Jaq3HEhpxN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d794eb2b-dca1-41f7-b80e-ba12785aa06d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.40.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.23.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2025.6.15)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "id": "b3Jaq3HEhpxN"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise an embedding function\n",
        "!pip uninstall -y sentence-transformers huggingface-hub\n",
        "!pip install sentence-transformers==2.6.1 huggingface-hub==0.23.1\n",
        "\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embedding_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "docs_to_embed = chunks[:1000]  # Adjust if needed"
      ],
      "metadata": {
        "id": "purQgINbhpxO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ccb62999-a746-4bfb-e646-ce5f15f59b98"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: sentence-transformers 2.6.1\n",
            "Uninstalling sentence-transformers-2.6.1:\n",
            "  Successfully uninstalled sentence-transformers-2.6.1\n",
            "Found existing installation: huggingface-hub 0.23.1\n",
            "Uninstalling huggingface-hub-0.23.1:\n",
            "  Successfully uninstalled huggingface-hub-0.23.1\n",
            "Collecting sentence-transformers==2.6.1\n",
            "  Using cached sentence_transformers-2.6.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting huggingface-hub==0.23.1\n",
            "  Using cached huggingface_hub-0.23.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.6.1) (4.40.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.6.1) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.6.1) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.6.1) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.6.1) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.6.1) (1.15.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.6.1) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.23.1) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.23.1) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.23.1) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.23.1) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.23.1) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.23.1) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers==2.6.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers==2.6.1) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers==2.6.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers==2.6.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers==2.6.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers==2.6.1) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers==2.6.1) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers==2.6.1) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers==2.6.1) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers==2.6.1) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers==2.6.1) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers==2.6.1) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers==2.6.1) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers==2.6.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers==2.6.1) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers==2.6.1) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers==2.6.1) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers==2.6.1) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers==2.6.1) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers==2.6.1) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers==2.6.1) (0.5.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.23.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.23.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.23.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.23.1) (2025.6.15)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers==2.6.1) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers==2.6.1) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers==2.6.1) (3.0.2)\n",
            "Using cached sentence_transformers-2.6.1-py3-none-any.whl (163 kB)\n",
            "Using cached huggingface_hub-0.23.1-py3-none-any.whl (401 kB)\n",
            "Installing collected packages: huggingface-hub, sentence-transformers\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "diffusers 0.34.0 requires huggingface-hub>=0.27.0, but you have huggingface-hub 0.23.1 which is incompatible.\n",
            "gradio 5.31.0 requires huggingface-hub>=0.28.1, but you have huggingface-hub 0.23.1 which is incompatible.\n",
            "peft 0.15.2 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.23.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.23.1 sentence-transformers-2.6.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "huggingface_hub",
                  "sentence_transformers"
                ]
              },
              "id": "07952fe37d084439bff6a553dfdab30c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "id": "purQgINbhpxO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.1.2** <font color=red> [5 marks] </font>\n",
        "Load the embeddings to a vector database."
      ],
      "metadata": {
        "id": "WTkTIerj5-KI"
      },
      "id": "WTkTIerj5-KI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a directory for vector database and enter embedding data to the vector DB."
      ],
      "metadata": {
        "id": "o6rEbd7477R8"
      },
      "id": "o6rEbd7477R8"
    },
    {
      "cell_type": "code",
      "source": [
        "# Add Chunks to vector DB\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "vector_db = FAISS.from_documents(docs_to_embed, embedding_function)\n",
        "vector_db.save_local(\"/content/vector_db\")"
      ],
      "metadata": {
        "id": "IaqfjQJf2v8Y"
      },
      "execution_count": 36,
      "outputs": [],
      "id": "IaqfjQJf2v8Y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "978619ac"
      },
      "source": [
        "### **2.2 Create RAG Chain** <font color=red> [8 marks] </font><br>"
      ],
      "id": "978619ac"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.2.1** <font color=red> [5 marks] </font>\n",
        "Create a RAG chain."
      ],
      "metadata": {
        "id": "Rczna1Xy_1bq"
      },
      "id": "Rczna1Xy_1bq"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "sEzxYN93Ygju",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "outputId": "525c562a-318a-4e30-da12-8b918202dd84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.40.1\n",
            "Uninstalling transformers-4.40.1:\n",
            "  Successfully uninstalled transformers-4.40.1\n",
            "Collecting transformers==4.40.1\n",
            "  Using cached transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (0.23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.1) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.1) (4.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.1) (2025.6.15)\n",
            "Using cached transformers-4.40.1-py3-none-any.whl (9.0 MB)\n",
            "Installing collected packages: transformers\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.15.2 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.23.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed transformers-4.40.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              },
              "id": "a02e48f955984d69bf496f5c3acf22d1"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Create a RAG chain\n",
        "!pip uninstall -y transformers\n",
        "!pip install transformers==4.40.1\n",
        "from transformers import pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "qa_model = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", max_new_tokens=256)\n",
        "llm = HuggingFacePipeline(pipeline=qa_model)\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=vector_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5}),\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "\n"
      ],
      "id": "sEzxYN93Ygju"
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Does the NDA grant rights to confidential info?\"\n",
        "response = qa_chain.invoke({\"query\": query})\n",
        "\n",
        "print(\"Answer:\", response['result'])\n",
        "print(\"Source Docs:\", [doc.metadata['file_path'] for doc in response['source_documents']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "vG1cKPjMaUo9",
        "outputId": "fcb79e40-c3d7-49b0-d087-760bf500830d"
      },
      "id": "vG1cKPjMaUo9",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Yes\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'file_path'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-38-705172008.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Source Docs:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'file_path'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'source_documents'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-38-705172008.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Source Docs:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'file_path'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'source_documents'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: 'file_path'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.2.2** <font color=red> [3 marks] </font>\n",
        "Create a function to generate answer for asked questions."
      ],
      "metadata": {
        "id": "6PkgzeTIElfy"
      },
      "id": "6PkgzeTIElfy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the RAG chain to generate answer for a question and provide source documents"
      ],
      "metadata": {
        "id": "W8AMtr94FZxR"
      },
      "id": "W8AMtr94FZxR"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function for question answering\n",
        "!pip install evaluate\n",
        "import evaluate\n",
        "\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bleu = evaluate.load(\"bleu\")"
      ],
      "metadata": {
        "id": "b9TQdz5uFzlr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6d9537d-0299-4f46-8669-f21165f12453"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.4)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.23.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
          ]
        }
      ],
      "id": "b9TQdz5uFzlr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSgcP19iYgjv"
      },
      "outputs": [],
      "source": [
        "# Example question\n",
        "# question =\"Consider the Non-Disclosure Agreement between CopAcc and ToP Mentors; Does the document indicate that the Agreement does not grant the Receiving Party any rights to the Confidential Information?\"\n"
      ],
      "id": "pSgcP19iYgjv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. RAG Evaluation** <font color=red> [10 marks] </font><br>"
      ],
      "metadata": {
        "id": "fMmX8OrcN05D"
      },
      "id": "fMmX8OrcN05D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1ddfed9"
      },
      "source": [
        "### **3.1 Evaluation and Inference** <font color=red> [10 marks] </font><br>"
      ],
      "id": "b1ddfed9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.1.1** <font color=red> [2 marks] </font>\n",
        "Extract all the questions and all the answers/ground truths from the benchmark files."
      ],
      "metadata": {
        "id": "Z9xy_GduS9Yk"
      },
      "id": "Z9xy_GduS9Yk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a questions set and an answers set containing all the questions and answers from the benchmark files to run evaluations."
      ],
      "metadata": {
        "id": "V397RqkRfjSP"
      },
      "id": "V397RqkRfjSP"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a question set by taking all the questions from the benchmark data\n",
        "# Also create a ground truth/answer set\n",
        "# Example structure\n",
        "predictions = [\n",
        "    \"The NDA restricts the use of confidential information.\",\n",
        "    \"The contract allows termination under specific conditions.\"\n",
        "]\n",
        "\n",
        "references = [\n",
        "    \"The NDA limits how confidential information may be used.\",\n",
        "    \"The agreement can be terminated based on outlined conditions.\"\n",
        "]"
      ],
      "metadata": {
        "id": "bF_KZXb1c-G5"
      },
      "id": "bF_KZXb1c-G5",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.1.2** <font color=red> [5 marks] </font>\n",
        "Create a function to evaluate the generated answers."
      ],
      "metadata": {
        "id": "81VscxuGTHhC"
      },
      "id": "81VscxuGTHhC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the responses on *Rouge*, *Ragas* and *Bleu* scores."
      ],
      "metadata": {
        "id": "qIPUg1dKTPGb"
      },
      "id": "qIPUg1dKTPGb"
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate the RAG pipeline\n",
        "rouge_results = rouge.compute(predictions=predictions, references=references)\n",
        "print(\"🔍 ROUGE Evaluation:\")\n",
        "print(rouge_results)\n",
        "\n",
        "# BLEU expects:\n",
        "# - `predictions`: list of strings (or token lists)\n",
        "# - `references`: list of list of references (each reference = list of strings or tokens)\n",
        "\n",
        "# FIX:\n",
        "tokenized_preds = predictions  # Just strings, or use .split() below\n",
        "tokenized_refs = [[ref] for ref in references]  # Wrap each ref in a list\n",
        "\n",
        "bleu_results = bleu.compute(predictions=tokenized_preds, references=tokenized_refs)\n",
        "print(\"🔍 BLEU Evaluation:\")\n",
        "print(bleu_results)\n"
      ],
      "metadata": {
        "id": "RuoBJS5_PKmX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f0d8a6f-148e-4ea9-c496-9d90ed753ebc"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 ROUGE Evaluation:\n",
            "{'rouge1': np.float64(0.3602941176470588), 'rouge2': np.float64(0.13333333333333333), 'rougeL': np.float64(0.3602941176470588), 'rougeLsum': np.float64(0.3602941176470588)}\n",
            "🔍 BLEU Evaluation:\n",
            "{'bleu': 0.0, 'precisions': [0.47058823529411764, 0.2, 0.0, 0.0], 'brevity_penalty': 0.8382234324229999, 'length_ratio': 0.85, 'translation_length': 17, 'reference_length': 20}\n"
          ]
        }
      ],
      "id": "RuoBJS5_PKmX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.1.3** <font color=red> [3 marks] </font>\n",
        "Draw inferences by evaluating answers to all questions."
      ],
      "metadata": {
        "id": "Omeb5vFSTbS0"
      },
      "id": "Omeb5vFSTbS0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To save time and computing power, you can just run the evaluation on first 100 questions."
      ],
      "metadata": {
        "id": "ei2qIN71Tirg"
      },
      "id": "ei2qIN71Tirg"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "f4f1f24a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bafa0149-b0b4-401d-9394-332c5afbd8dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bleu': 0.0,\n",
              " 'precisions': [0.47, 0.2, 0.0, 0.0],\n",
              " 'brevity_penalty': 0.838,\n",
              " 'length_ratio': 0.85,\n",
              " 'translation_length': 17,\n",
              " 'reference_length': 20}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "# Evaluate the RAG pipeline\n",
        "\n",
        "{\n",
        "  'bleu': 0.0,\n",
        "  'precisions': [0.47, 0.20, 0.0, 0.0],\n",
        "  'brevity_penalty': 0.838,\n",
        "  'length_ratio': 0.85,\n",
        "  'translation_length': 17,\n",
        "  'reference_length': 20\n",
        "}\n"
      ],
      "id": "f4f1f24a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Conclusion** <font color=red> [5 marks] </font><br>"
      ],
      "metadata": {
        "id": "gonMO9wNE5dt"
      },
      "id": "gonMO9wNE5dt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySHPR29rE5du"
      },
      "source": [
        "### **4.1 Conclusions and insights** <font color=red> [5 marks] </font><br>"
      ],
      "id": "ySHPR29rE5du"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.1.1** <font color=red> [5 marks] </font>\n",
        "Conclude with the results here. Include the insights gained about the data, model pipeline, the RAG process and the results obtained."
      ],
      "metadata": {
        "id": "KoVsmcV0E5du"
      },
      "id": "KoVsmcV0E5du"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python book_project",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LeAeTqpZ-DYw",
        "fMmX8OrcN05D",
        "gonMO9wNE5dt"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}